{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# callbacks\n",
    "\n",
    "> Lightning callbacks for checkpoint and log management via ProjectIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nfrom __future__ import annotations\nfrom pathlib import Path\nfrom typing import Any, Optional\n\ntry:\n    from lightning.pytorch.callbacks import Callback, ModelCheckpoint\n    from lightning.pytorch import Trainer, LightningModule\n    HAS_LIGHTNING = True\nexcept ImportError:\n    # Lightning is optional - provide stub for type hints\n    HAS_LIGHTNING = False\n    class Callback:  # type: ignore\n        \"\"\"Stub callback for when Lightning is not installed.\"\"\"\n        pass\n    class ModelCheckpoint:  # type: ignore\n        pass\n    Trainer = Any  # type: ignore\n    LightningModule = Any  # type: ignore\n\nfrom projio.core import ProjectIO, PIO"
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## IOCheckpointCallback\n",
    "\n",
    "A callback that uses ProjectIO to place checkpoints and optionally tracks producers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IOCheckpointCallback(Callback):\n",
    "    \"\"\"Lightning callback that uses ProjectIO for checkpoint paths.\n",
    "    \n",
    "    This callback integrates with ProjectIO to:\n",
    "    - Place checkpoints in the configured checkpoint directory\n",
    "    - Apply datestamp prefixes/directories as configured\n",
    "    - Optionally track which training script produced each checkpoint\n",
    "    \n",
    "    Parameters:\n",
    "        io: ProjectIO instance (default: creates new one).\n",
    "        run: Run name for subdirectory organization.\n",
    "        filename: Checkpoint filename template with {epoch}, {step}, etc.\n",
    "        datestamp: Override datestamp behavior.\n",
    "        track_producer: Record producer info for checkpoints.\n",
    "        producer_script: Script path to record as producer.\n",
    "    \n",
    "    Example:\n",
    "        >>> callback = IOCheckpointCallback(run=\"experiment_1\")\n",
    "        >>> trainer = Trainer(callbacks=[callback])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        io: ProjectIO | None = None,\n",
    "        run: str | None = None,\n",
    "        filename: str = \"{epoch:02d}-{step:06d}\",\n",
    "        datestamp: bool | None = None,\n",
    "        track_producer: bool = False,\n",
    "        producer_script: str | Path | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.io = io or ProjectIO()\n",
    "        self.run = run\n",
    "        self.filename = filename\n",
    "        self.datestamp = datestamp\n",
    "        self.track_producer = track_producer\n",
    "        self.producer_script = Path(producer_script) if producer_script else None\n",
    "        self.cached_checkpoint_dir: Path | None = None\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_dir(self) -> Path:\n",
    "        \"\"\"Get the checkpoint directory for this callback.\"\"\"\n",
    "        if self.cached_checkpoint_dir is None:\n",
    "            self.cached_checkpoint_dir = self.io.tensorboard_run(\n",
    "                run=self.run,\n",
    "                datestamp=self.datestamp\n",
    "            ).parent.parent / \"checkpoints\"\n",
    "            if self.run:\n",
    "                self.cached_checkpoint_dir = self.cached_checkpoint_dir / self.run\n",
    "        return self.cached_checkpoint_dir\n",
    "    \n",
    "    def get_checkpoint_path(self, epoch: int, step: int, ext: str = \".ckpt\") -> Path:\n",
    "        \"\"\"Build checkpoint path for given epoch and step.\n",
    "        \n",
    "        Parameters:\n",
    "            epoch: Current epoch number.\n",
    "            step: Current global step.\n",
    "            ext: File extension.\n",
    "            \n",
    "        Returns:\n",
    "            Full path to checkpoint file.\n",
    "        \"\"\"\n",
    "        name = self.filename.format(epoch=epoch, step=step)\n",
    "        return self.io.checkpoint_path(\n",
    "            name=name,\n",
    "            ext=ext,\n",
    "            run=self.run,\n",
    "            datestamp=self.datestamp\n",
    "        )\n",
    "    \n",
    "    def on_train_start(self, trainer: \"Trainer\", pl_module: \"LightningModule\") -> None:\n",
    "        \"\"\"Called when training starts - ensures checkpoint directory exists.\"\"\"\n",
    "        if not HAS_LIGHTNING:\n",
    "            return\n",
    "        # Pre-create the checkpoint directory\n",
    "        _ = self.checkpoint_dir\n",
    "    \n",
    "    def on_save_checkpoint(\n",
    "        self,\n",
    "        trainer: \"Trainer\",\n",
    "        pl_module: \"LightningModule\",\n",
    "        checkpoint: dict\n",
    "    ) -> None:\n",
    "        \"\"\"Called when a checkpoint is saved - track producer if enabled.\"\"\"\n",
    "        if not HAS_LIGHTNING or not self.track_producer:\n",
    "            return\n",
    "        if self.producer_script:\n",
    "            # Get the checkpoint path that will be saved\n",
    "            epoch = trainer.current_epoch\n",
    "            step = trainer.global_step\n",
    "            ckpt_path = self.get_checkpoint_path(epoch, step)\n",
    "            self.io.track_producer(\n",
    "                target=ckpt_path,\n",
    "                producer=self.producer_script,\n",
    "                kind=\"checkpoint\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## IOLogCallback\n",
    "\n",
    "A callback that routes logs and tensorboard runs through ProjectIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IOLogCallback(Callback):\n",
    "    \"\"\"Lightning callback that routes logs through ProjectIO.\n",
    "    \n",
    "    This callback integrates with ProjectIO to:\n",
    "    - Set up TensorBoard log directories\n",
    "    - Apply datestamp prefixes/directories as configured\n",
    "    - Provide consistent logging paths across experiments\n",
    "    \n",
    "    Parameters:\n",
    "        io: ProjectIO instance (default: creates new one).\n",
    "        run: Run name for subdirectory organization.\n",
    "        datestamp: Override datestamp behavior.\n",
    "    \n",
    "    Example:\n",
    "        >>> callback = IOLogCallback(run=\"experiment_1\")\n",
    "        >>> trainer = Trainer(callbacks=[callback])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        io: ProjectIO | None = None,\n",
    "        run: str | None = None,\n",
    "        datestamp: bool | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.io = io or ProjectIO()\n",
    "        self.run = run\n",
    "        self.datestamp = datestamp\n",
    "        self.cached_log_dir: Path | None = None\n",
    "    \n",
    "    @property\n",
    "    def log_dir(self) -> Path:\n",
    "        \"\"\"Get the TensorBoard log directory for this callback.\"\"\"\n",
    "        if self.cached_log_dir is None:\n",
    "            self.cached_log_dir = self.io.tensorboard_run(\n",
    "                run=self.run,\n",
    "                datestamp=self.datestamp\n",
    "            )\n",
    "        return self.cached_log_dir\n",
    "    \n",
    "    def on_train_start(self, trainer: \"Trainer\", pl_module: \"LightningModule\") -> None:\n",
    "        \"\"\"Called when training starts - configure trainer log directory.\"\"\"\n",
    "        if not HAS_LIGHTNING:\n",
    "            return\n",
    "        # Ensure log directory exists\n",
    "        log_dir = self.log_dir\n",
    "        \n",
    "        # Try to configure the trainer's logger if it has a log_dir attribute\n",
    "        if hasattr(trainer, 'logger') and trainer.logger is not None:\n",
    "            logger = trainer.logger\n",
    "            if hasattr(logger, 'log_dir'):\n",
    "                # Some loggers allow setting log_dir (external Lightning API)\n",
    "                try:\n",
    "                    setattr(logger, 'log_dir', str(log_dir))\n",
    "                except (AttributeError, TypeError):\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Integration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using both callbacks together\n",
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    io = ProjectIO(root=tmp, use_datestamp=False, auto_create=True)\n",
    "    \n",
    "    ckpt_cb = IOCheckpointCallback(io=io, run=\"exp1\")\n",
    "    log_cb = IOLogCallback(io=io, run=\"exp1\")\n",
    "    \n",
    "    print(f\"Checkpoint dir: {ckpt_cb.checkpoint_dir}\")\n",
    "    print(f\"Log dir: {log_cb.log_dir}\")\n",
    "    print(f\"Checkpoint path (epoch 5, step 1000): {ckpt_cb.get_checkpoint_path(5, 1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
